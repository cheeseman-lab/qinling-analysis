# Brieflow Google Batch configuration file
# This file configures how Snakemake submits jobs to Google Cloud Batch
#
# IMPORTANT: Update the project-specific settings below before running!

# Specify the executor
executor: googlebatch

# ============================================================================
# PROJECT-SPECIFIC SETTINGS - UPDATE THESE!
# ============================================================================

# Google Batch specific settings
googlebatch-project: lasagna-199723                    # Your GCP project ID
googlebatch-region: us-west1                           # GCP region (us-west1, us-central1, etc.)
googlebatch-service-account: batch-sa@lasagna-199723.iam.gserviceaccount.com  # Service account email

# Storage settings - GCS configuration
storage-gcs-project: lasagna-199723                    # Your GCP project ID
default-storage-prefix: gs://scale1  # Your GCS bucket

# ============================================================================
# CONTAINER CONFIGURATION - Usually don't need to change
# ============================================================================

# Container OS settings - use Container-Optimized OS
googlebatch-image-family: "batch-cos-stable-official"
googlebatch-image-project: "batch-custom-image"

# Specify the container image that contains brieflow and all dependencies
googlebatch-container: "us-west1-docker.pkg.dev/lasagna-199723/brieflow-repo/brieflow:latest"

# Use the custom docker-entrypoint.sh to activate conda environment
googlebatch-entrypoint: "/docker-entrypoint.sh"

# CRITICAL: Tell executor dependencies are pre-installed (skips runtime pip install)
# This prevents dependency version conflicts that cause silent failures
googlebatch-container-dependencies-installed: true

# ============================================================================
# STORAGE AND FILESYSTEM SETTINGS
# ============================================================================

# Use GCS as the default storage provider
default-storage-provider: gcs

# Keep these on the controller's local filesystem for speed
# IMPORTANT: Including 'sources' packages workflow code with each batch job
# CRITICAL: 'input-output' prevents expensive GCS API calls during DAG building (saves 5+ hours!)
shared-fs-usage:
  - persistence           # Keep metadata/cache local for performance
  - source-cache          # Downloaded packages stay on controller
  - sources               # Package workflow code with each batch job
  - input-output          # CRITICAL! Tells Snakemake input/output files accessible via GCS - prevents 21,552+ API calls!
  - storage-local-copies  # Helps with caching

# Storage settings for controller VM - where local files are stored
local-storage-prefix: /mnt/data/blainey/qinling-analysis

# ============================================================================
# JOB EXECUTION SETTINGS
# ============================================================================

# Maximum number of concurrent jobs (unlimited for Google Batch auto-scaling)
jobs: unlimited

# ============================================================================
# RESOURCE ALLOCATION
# ============================================================================

# Default resources for all rules
# NOTE: Increase default mem_mb to reduce need for per-rule overrides
default-resources:
  mem_mb: 5000                              # Default memory (MB)
  cpus_per_task: 1                          # Default CPU cores
  runtime: 400                              # Max runtime in minutes
  googlebatch_machine_type: "n2-standard-4" # Default VM type (4 vCPU, 16 GB RAM) - faster provisioning than c2
  googlebatch_boot_disk_gb: 30              # Boot disk size (GB)
  googlebatch_retry_count: 1                # Retry failed jobs once
  googlebatch_max_run_duration: "3600s"     # Max run duration (1 hour)

# Override resources for specific rules
# IMPORTANT: Keep this list minimal to avoid command-line length issues
# Only specify resources that differ significantly from defaults
set-resources:
    # High memory jobs (20 GB)
    combine_reads:
        mem_mb: 20000
    combine_cells:
        mem_mb: 20000
    combine_sbs_info:
        mem_mb: 20000
    combine_phenotype_info:
        mem_mb: 20000
    fast_alignment:
        mem_mb: 20000
        cpus_per_task: 4
    merge:
        mem_mb: 20000
        cpus_per_task: 4
    format_merge:
        mem_mb: 20000
    eval_merge:
        mem_mb: 20000
    clean_merge:
        mem_mb: 20000
    deduplicate_merge:
        mem_mb: 20000
        cpus_per_task: 4

    # Very high memory jobs (50-100 GB)
    calculate_ic_sbs:
        mem_mb: 50000
    eval_segmentation_sbs:
        mem_mb: 50000
    eval_mapping:
        mem_mb: 50000
        cpus_per_task: 12
    eval_segmentation_phenotype:
        mem_mb: 50000
    filter:
        mem_mb: 100000
    prepare_montage_data:
        mem_mb: 100000
        cpus_per_task: 8

    # Extremely high memory jobs (200-980 GB)
    split_datasets:
        mem_mb: 200000
    merge_phenotype_cp:
        mem_mb: 300000
    calculate_ic_phenotype:
        mem_mb: 500000
    final_merge:
        mem_mb: 500000
    generate_feature_table:
        mem_mb: 980000
        cpus_per_task: 4
    align:
        mem_mb: 980000
        cpus_per_task: 4
    aggregate:
        mem_mb: 980000
        cpus_per_task: 4
    eval_aggregate:
        mem_mb: 980000
        cpus_per_task: 4

    # Multi-CPU jobs
    segment_sbs:
        cpus_per_task: 4
    segment_phenotype:
        cpus_per_task: 4
    phate_leiden_clustering:
        mem_mb: 10000
    generate_montage:
        mem_mb: 10000

# ============================================================================
# COST OPTIMIZATION (Optional)
# ============================================================================

# Uncomment to use preemptible VMs (up to 80% cost savings)
# Trade-off: Jobs may be interrupted, but will retry
# default-resources:
#   googlebatch_provisioning_model: "PREEMPTIBLE"
