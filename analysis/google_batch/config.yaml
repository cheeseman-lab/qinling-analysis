# Brieflow Google Batch configuration file
# This file configures how Snakemake submits jobs to Google Cloud Batch

# ============================================================================
# EXECUTOR SETTINGS
# ============================================================================

executor: googlebatch
jobs: unlimited

# ============================================================================
# PROJECT-SPECIFIC SETTINGS
# ============================================================================

googlebatch-project: lasagna-199723
googlebatch-region: us-west1
googlebatch-service-account: batch-sa@lasagna-199723.iam.gserviceaccount.com

# ============================================================================
# STORAGE SETTINGS
# ============================================================================

default-storage-provider: gcs
default-storage-prefix: gs://scale1
storage-gcs-project: lasagna-199723
local-storage-prefix: /mnt/data/blainey/qinling-analysis

# CRITICAL: 'input-output' prevents 21,552+ GCS API calls during DAG building!
shared-fs-usage:
  - persistence
  - source-cache
  - sources
  - input-output
  - storage-local-copies

# ============================================================================
# CONTAINER CONFIGURATION
# ============================================================================

googlebatch-image-family: "batch-cos-stable-official"
googlebatch-image-project: "batch-custom-image"
googlebatch-container: "us-west1-docker.pkg.dev/lasagna-199723/brieflow-repo/brieflow:latest"
googlebatch-entrypoint: "/docker-entrypoint.sh"
googlebatch-container-dependencies-installed: true

# ============================================================================
# DAG BUILDING OPTIMIZATIONS (must be set via command line, not config)
# ============================================================================
# These settings CANNOT be set in config.yaml - they must be command-line flags:
# --latency-wait 0
# --max-inventory-time 300
# --ignore-incomplete
# --max-checksum-file-size 0

# ============================================================================
# JOB SUBMISSION OPTIMIZATIONS
# ============================================================================

# Rate limiting (these CAN be set in config)
max-jobs-per-timespan: "10000/1s"
max-status-checks-per-second: 1
seconds-between-status-checks: 30

# Preemptible settings (these CAN be set in config)
preemptible-rules: []  # Empty list means ALL rules are preemptible
preemptible-retries: 3

# ============================================================================
# EXECUTION SETTINGS
# ============================================================================

rerun-triggers:
  - mtime
rerun-incomplete: true

# ============================================================================
# RESOURCE ALLOCATION
# ============================================================================

default-resources:
  mem_mb: 5000
  cpus_per_task: 1
  runtime: 400
  googlebatch_machine_type: "n2-standard-4"
  googlebatch_boot_disk_gb: 30
  googlebatch_retry_count: 3
  googlebatch_max_run_duration: "3600s"

# ============================================================================
# PER-RULE RESOURCE OVERRIDES
# Keep minimal to avoid command-line length issues
# ============================================================================

set-resources:
  # High memory jobs (20 GB)
  combine_reads:
    mem_mb: 20000
  combine_cells:
    mem_mb: 20000
  combine_sbs_info:
    mem_mb: 20000
  combine_phenotype_info:
    mem_mb: 20000
  fast_alignment:
    mem_mb: 20000
    cpus_per_task: 4
  merge:
    mem_mb: 20000
    cpus_per_task: 4
  format_merge:
    mem_mb: 20000
  eval_merge:
    mem_mb: 20000
  clean_merge:
    mem_mb: 20000
  deduplicate_merge:
    mem_mb: 20000
    cpus_per_task: 4

  # Very high memory jobs (50-100 GB)
  calculate_ic_sbs:
    mem_mb: 50000
  eval_segmentation_sbs:
    mem_mb: 50000
  eval_mapping:
    mem_mb: 50000
    cpus_per_task: 12
  eval_segmentation_phenotype:
    mem_mb: 50000
  filter:
    mem_mb: 100000
  prepare_montage_data:
    mem_mb: 100000
    cpus_per_task: 8

  # Extremely high memory jobs (200-980 GB)
  split_datasets:
    mem_mb: 200000
  merge_phenotype_cp:
    mem_mb: 300000
  calculate_ic_phenotype:
    mem_mb: 500000
  final_merge:
    mem_mb: 500000
  generate_feature_table:
    mem_mb: 980000
    cpus_per_task: 4
  align:
    mem_mb: 980000
    cpus_per_task: 4
  aggregate:
    mem_mb: 980000
    cpus_per_task: 4
  eval_aggregate:
    mem_mb: 980000
    cpus_per_task: 4

  # Multi-CPU jobs
  segment_sbs:
    cpus_per_task: 4
  segment_phenotype:
    cpus_per_task: 4
  phate_leiden_clustering:
    mem_mb: 10000
  generate_montage:
    mem_mb: 10000